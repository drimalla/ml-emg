{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the data: returning data and filters for stimulus and baseline for both muscles\n",
    "def get_data(base=1, method='subtraction', zstand=1, resolution=100, outl='sd', exclude='no'):\n",
    "   \n",
    "    #Function for Outlier-Detection\n",
    "    def outlier_trials(df, outl):\n",
    "        for i in set(df.vpn.values):\n",
    "            \n",
    "            target=[zyg_s, cor_s, scl_s]\n",
    "\n",
    "            for t in target:\n",
    "                \n",
    "                werte=np.array(df.loc[(df.vpn==i), t]) \n",
    "                         \n",
    "                trial_mean=np.nanmean(werte, axis=1)\n",
    "                vpn_mean=np.nanmean(trial_mean, axis=0)            \n",
    "                vpn_std=np.nanstd(trial_mean, axis=0)\n",
    "\n",
    "                if outl=='sd':\n",
    "                    out_ind=(np.abs(trial_mean-vpn_mean))>3*vpn_std\n",
    "                    \n",
    "                if outl=='iq':\n",
    "                    quartile_1, quartile_3 = np.percentile(trial_mean, [25, 75])\n",
    "                    iqr = quartile_3 - quartile_1\n",
    "                    lower_bound = quartile_1 - (iqr * 3)\n",
    "                    upper_bound = quartile_3 + (iqr * 3)\n",
    "                    out_ind=(trial_mean > upper_bound) | (trial_mean < lower_bound)\n",
    "                werte[out_ind]=np.nan   \n",
    "                df.loc[(df.vpn==i), t]=werte\n",
    "               \n",
    "        return df\n",
    "\n",
    "    def outlier(df, outl):\n",
    "        target=[zyg_s, zyg_b, cor_b, cor_s, scl_b, scl_s]\n",
    "        for i in set(df.vpn.values):\n",
    "            \n",
    "            for t in target:\n",
    "                werte=np.array(df.loc[(df.vpn==i), t])\n",
    "                rows=len(werte)\n",
    "                columns=len(werte.T)\n",
    "                werte_r=werte.reshape(1,(rows*columns))\n",
    "                \n",
    "                if outl=='iq':\n",
    "                    quartile_1, quartile_3 = np.percentile(werte_r, [25, 75])\n",
    "                    iqr = quartile_3 - quartile_1\n",
    "                    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "                    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "                    werte_r[np.where((werte_r > upper_bound) | (werte_r < lower_bound))]=np.nan\n",
    "                if outl=='sd':\n",
    "                    multi=3\n",
    "                    werte_r[np.abs(werte_r-np.nanmean(werte_r))>multi*np.nanstd(werte_r)]=np.nan\n",
    "                \n",
    "                werte_o=werte_r    \n",
    "                werte_new=werte_o.reshape(rows,columns)  \n",
    "                df.loc[(df.vpn==i), t]=werte_new  \n",
    "        return df\n",
    "    \n",
    "    #Function for Z-Standardisation           \n",
    "    def mimstand(df):\n",
    "        vpn=df.vpn.unique()\n",
    "            \n",
    "        for i in vpn:\n",
    "            target=[zyg_s, cor_s, scl_s] \n",
    "            for t in target:\n",
    "                orig=df.loc[(df.vpn==i),t]\n",
    "                orig_mu=orig.mean().mean()\n",
    "                orig_stand=orig.std().mean()\n",
    "                zdata=(orig-orig_mu)/orig_stand\n",
    "                df.loc[(df.vpn==i),t]=zdata \n",
    "        return df\n",
    "    \n",
    "    # Loading of the Datafile (3 possible resolutions and naming of the columns)\n",
    "    if resolution==50:\n",
    "        filename='MET50_af.csv'\n",
    "        df=pd.read_csv(filename,sep=',', na_values=['?'])   \n",
    "        #Baseline = 0-1000ms\n",
    "        scl_b=df.loc[:,'METSCL1':'METSCL19'].columns\n",
    "        #Stimulus = 500ms-4000ms after trigger\n",
    "        scl_s=df.loc[:,'METSCL40':'METSCL110'].columns\n",
    "        \n",
    "    if (resolution==500):\n",
    "        filename='MET500_af.csv' \n",
    "        df=pd.read_csv(filename,sep=',', na_values=['?'])\n",
    "             \n",
    "        if base==1:\n",
    "            print 'base: 0ms-500ms'\n",
    "            zyg_b=df.loc[:,'METZYG1':'METZYG1'].columns       \n",
    "            cor_b=df.loc[:,'METCOR1':'METCOR1'].columns\n",
    "            scl_b=df.loc[:,'METSCL1':'METSCL1'].columns\n",
    "        if base==2:\n",
    "            print 'base: 0ms-1000ms'\n",
    "            zyg_b=df.loc[:,'METZYG1':'METZYG2'].columns\n",
    "            cor_b=df.loc[:,'METCOR1':'METCOR2'].columns\n",
    "            scl_b=df.loc[:,'METSCL1':'METSCL2'].columns  \n",
    "        if base==3:\n",
    "            print 'base 500ms-1000ms'\n",
    "            zyg_b=df.loc[:,'METZYG2':'METZYG2'].columns\n",
    "            cor_b=df.loc[:,'METCOR2':'METCOR2'].columns         \n",
    "            scl_b=df.loc[:,'METSCL2':'METSCL2'].columns\n",
    "            \n",
    "        zyg_s=df.loc[:,'METZYG5':'METZYG11'].columns\n",
    "        cor_s=df.loc[:,'METCOR5':'METCOR11'].columns\n",
    "        scl_s=df.loc[:,'METSCL5':'METSCL11'].columns\n",
    "        \n",
    "    if (resolution==100):\n",
    "        filename='MET100_af.csv'     \n",
    "        df=pd.read_csv(filename,sep=',', na_values=['?'])\n",
    "             \n",
    "        if base==1:\n",
    "            print 'base: 0ms-500ms'\n",
    "            zyg_b=df.loc[:,'METZYG1':'METZYG5'].columns       \n",
    "            cor_b=df.loc[:,'METCOR1':'METCOR5'].columns\n",
    "            scl_b=df.loc[:,'METSCL1':'METSCL5'].columns\n",
    "        if base==2:\n",
    "            print 'base: 0ms-1000ms'\n",
    "            zyg_b=df.loc[:,'METZYG1':'METZYG9'].columns\n",
    "            cor_b=df.loc[:,'METCOR1':'METCOR9'].columns\n",
    "            scl_b=df.loc[:,'METSCL1':'METSCL9'].columns\n",
    "        if base==3:\n",
    "            print 'base 500ms-1000ms'\n",
    "            zyg_b=df.loc[:,'METZYG5':'METZYG9'].columns\n",
    "            cor_b=df.loc[:,'METCOR5':'METCOR9'].columns\n",
    "            scl_b=df.loc[:,'METSCL5':'METSCL9'].columns\n",
    "            \n",
    "        scl_s=df.loc[:,'METSCL20':'METSCL55'].columns\n",
    "        zyg_s=df.loc[:,'METZYG20':'METZYG55'].columns\n",
    "        cor_s=df.loc[:,'METCOR20':'METCOR55'].columns   \n",
    "    \n",
    "    #exclude weird participant\n",
    "    if exclude=='yes':\n",
    "        df=df[df.vpn!=38].reset_index(drop=True)\n",
    "        df=df[df.vpn!=22].reset_index(drop=True)\n",
    "        df=df[df.vpn!=31].reset_index(drop=True)\n",
    "        df=df[df.vpn!=132].reset_index(drop=True)\n",
    "\n",
    "        \n",
    "    target=[zyg_s, zyg_b, cor_b, cor_s, scl_b, scl_s]\n",
    "    for t in target:\n",
    "        #NaN setzen von Trials, bei denen mehr als 25% NaNs\n",
    "        numberNaNs=df[t].isnull().sum(axis=1)\n",
    "        df.loc[(numberNaNs>(len(t)*0.30)), t]=np.nan\n",
    "\n",
    "    \n",
    "    #Baseline-Correction\n",
    "    # raw data\n",
    "    if method=='raw':\n",
    "        print 'raw data'\n",
    "    \n",
    "    #1500ms baseline\n",
    "    if method=='subtraction': \n",
    "        print 'baseline subtraction'\n",
    "        df[zyg_s]=df[zyg_s].sub(np.nanmean(df[zyg_b], axis=1), axis=0)\n",
    "        df[cor_s]=df[cor_s].sub(np.nanmean(df[cor_b], axis=1), axis=0)\n",
    "        \n",
    "        df[zyg_b]=df[zyg_b].sub(np.nanmean(df[zyg_b], axis=1), axis=0)\n",
    "        df[cor_b]=df[cor_b].sub(np.nanmean(df[cor_b], axis=1), axis=0)\n",
    "        \n",
    "        df[scl_s]=df[scl_s].sub(np.nanmean(df[scl_b], axis=1), axis=0)\n",
    "        \n",
    " \n",
    "    #Division durch Baseline\n",
    "    if method=='division':\n",
    "        print 'baseline division'\n",
    "        df[zyg_s]=df[zyg_s].divide(np.nanmean(df[zyg_b], axis=1), axis=0)\n",
    "        df[cor_s]=df[cor_s].divide(np.nanmean(df[cor_b], axis=1), axis=0)\n",
    "        \n",
    "        df[zyg_b]=df[zyg_b].divide(np.nanmean(df[zyg_b], axis=1), axis=0)\n",
    "        df[cor_b]=df[cor_b].divide(np.nanmean(df[cor_b], axis=1), axis=0)\n",
    "\n",
    "        df[scl_s]=df[scl_s].divide(np.nanmean(df[scl_b], axis=1), axis=0)\n",
    "\n",
    "\n",
    "    #Division durch Trial\n",
    "    if method=='trial_division':\n",
    "        print 'trial division'\n",
    "        df[zyg_s]=df[zyg_s].divide(np.nanmean(df[zyg_s], axis=1), axis=0)\n",
    "        df[cor_s]=df[cor_s].divide(np.nanmean(df[cor_s], axis=1), axis=0)\n",
    "        \n",
    "        df[zyg_b]=df[zyg_b].divide(np.nanmean(df[zyg_b], axis=1), axis=0)\n",
    "        df[cor_b]=df[cor_b].divide(np.nanmean(df[cor_b], axis=1), axis=0)\n",
    "        \n",
    "        df[scl_s]=df[scl_s].divide(np.nanmean(df[scl_s], axis=1), axis=0)\n",
    "\n",
    "    #Trial-Outlier-Detection\n",
    "    if outl=='orig':\n",
    "        print 'no outlier detection'\n",
    "    else:\n",
    "        df=outlier_trials(df, outl)\n",
    "        \n",
    "    #Z-standardisation       \n",
    "    if zstand==1:\n",
    "        df=mimstand(df)\n",
    "    \n",
    "    #make columns easier accesible with labels \n",
    "    fragebogen=['VPN', 'SPF_SUM', 'SPF_FS_SUM', 'SPF_EC_SUM', 'SPF_PT_SUM',\n",
    "       'SPF_PD_SUM', 'SPF_Empathy_SUM', 'AQsum', 'NPIgesamt', 'SOCsum',\n",
    "       'PANAS01pos', 'PANAS01neg', 'PANAS02pos', 'PANAS02neg']\n",
    "    MET_num=['key', 'time']\n",
    "    MET_cat=['con', 'cor', 'gen', 'age', 'val']\n",
    "    \n",
    "    df['all']=np.ones(len(df))\n",
    "    df['gender']=df.vpn>100\n",
    "    \n",
    "    #Exlude cases, where a participant has not answered a question\n",
    "    #df[df[fragebogen]==0]=np.nan\n",
    "    \n",
    "    #Name the triggers with conditions\n",
    "    df.loc[(df.triggers==101), 'triggers']='pos_cog'\n",
    "    df.loc[(df.triggers==102), 'triggers']='neg_emo'\n",
    "    df.loc[(df.triggers==103), 'triggers']='pos_emo'\n",
    "    df.loc[(df.triggers==104), 'triggers']='neg_cog'\n",
    "    df['all']=np.ones(len(df))\n",
    "     \n",
    "    return df, zyg_b, cor_b, zyg_s, cor_s, scl_s, scl_b, fragebogen, MET_num, MET_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(df, zyg_s, cor_s, scl_s):\n",
    "    \n",
    "    ## Physiological Measure\n",
    "    df['zyg_mean']=df[zyg_s].mean(axis=1)\n",
    "    df['cor_mean']=df[cor_s].mean(axis=1)\n",
    "    df['scl_mean']=df[scl_s].mean(axis=1)\n",
    "\n",
    "    df['zyg_max']=df[zyg_s].max(axis=1)\n",
    "    df['cor_max']=df[cor_s].max(axis=1)\n",
    "    df['scl_max']=df[scl_s].max(axis=1)\n",
    "\n",
    "    df['zyg_min']=df[zyg_s].min(axis=1)\n",
    "    df['cor_min']=df[cor_s].min(axis=1)\n",
    "    df['scl_min']=df[scl_s].min(axis=1)\n",
    "\n",
    "    df['zyg_var']=df[zyg_s].var(axis=1)\n",
    "    df['cor_var']=df[cor_s].var(axis=1)\n",
    "    df['scl_var']=df[cor_s].var(axis=1)\n",
    "\n",
    "    df['zyg_skew']=df[zyg_s].skew(axis=1)\n",
    "    df['cor_skew']=df[cor_s].skew(axis=1)\n",
    "    df['scl_skew']=df[cor_s].skew(axis=1)\n",
    "\n",
    "    df['zyg_kurt']=df[zyg_s].kurtosis(axis=1)\n",
    "    df['cor_kurt']=df[cor_s].kurtosis(axis=1)\n",
    "    df['scl_kurt']=df[scl_s].kurtosis(axis=1)\n",
    "\n",
    "    #Mimicry-Measures\n",
    "    neg_mim= np.nanmean((np.array(df[cor_s][df['val']=='neg'])-np.array(df[zyg_s][df['val']=='neg'])), dtype=np.float64, axis=1)\n",
    "    pos_mim= np.nanmean(np.array(df[zyg_s][df['val']=='pos'])-np.array(df[cor_s][df['val']=='pos']), dtype=np.float64, axis=1)\n",
    "    df['mim_mean']=np.zeros(len(df))\n",
    "    df.loc[(df['val']=='neg'), 'mim_mean']=neg_mim\n",
    "    df.loc[(df['val']=='pos'), 'mim_mean']=pos_mim\n",
    "\n",
    "    neg_mimMAX= np.max((np.array(df[cor_s][df['val']=='neg'])-np.array(df[zyg_s][df['val']=='neg'])), axis=1)\n",
    "    pos_mimMAX= np.max(np.array(df[zyg_s][df['val']=='pos'])-np.array(df[cor_s][df['val']=='pos']), axis=1)\n",
    "    df['mim_max']=np.zeros(len(df))\n",
    "    df.loc[(df['val']=='neg'), 'mim_max']=neg_mimMAX\n",
    "    df.loc[(df['val']=='pos'), 'mim_max']=pos_mimMAX\n",
    "\n",
    "    #Gender\n",
    "    df['female']=df.VPN>100\n",
    "\n",
    "    #Creating handcrafted Features\n",
    "    handcrafted=['zyg_mean', 'cor_mean', 'zyg_max', 'cor_max', 'zyg_min', 'cor_min', \n",
    "                          'zyg_var', 'cor_var', 'zyg_skew', 'cor_skew', 'zyg_kurt', 'cor_kurt']\n",
    "\n",
    "    ## Participants traits  \n",
    "    df['coge_vpn']=df.loc[(df.con==1),'cor'].groupby(df['vpn']).transform('mean')\n",
    "    df['emoe_vpn']=df.loc[(df.con==2),'key'].groupby(df['vpn']).transform('mean')\n",
    "    beh=['coge_vpn', 'emoe_vpn']\n",
    "    \n",
    "    ## Pic traits ## Participants traits  \n",
    "    df['coge_pic']=df.loc[(df.con==1),'cor'].groupby(df['pic']).transform('mean')\n",
    "    df['emoe_pic']=df.loc[(df.con==2),'key'].groupby(df['pic']).transform('mean')\n",
    "    df['rt_pic']=df.loc[(df.con==1),'time'].groupby(df['pic']).transform('mean')   \n",
    "    \n",
    "    #recode valence and condition labels\n",
    "    df.loc[(df['val']=='pos'), 'val']=1\n",
    "    df.loc[(df['val']=='neg'), 'val']=0\n",
    "    df.loc[(df['con']==2), 'con']=0\n",
    "    df['val']=pd.to_numeric(df.val)\n",
    "\n",
    "    return df, handcrafted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df, zyg_b, cor_b, zyg_s, cor_s, scl_s, scl_b, fragebogen, MET_num, MET_cat=get_data(base=2, \n",
    "                                                                                 zstand=1, \n",
    "                                                                                 method='subtraction', \n",
    "                                                                                 resolution=100,\n",
    "                                                                                 outl='sd', \n",
    "                                                                                 exclude='yes')\n",
    "\n",
    "df, handcrafted =prepare(df, zyg_s, cor_s, scl_s)\n",
    "\n",
    "#df=df.fillna(method='ffill') #exclude NaNs\n",
    "df=df.dropna(subset=zyg_s | cor_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(df.groupby('vpn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression (Function for Linear Regression and SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn import linear_model \n",
    "    \n",
    "def regression(X, y, vpn=pd.DataFrame(), cv=3, hypercv=5):\n",
    "    y_test = []\n",
    "    y_mean=[]\n",
    "    y_pred_lin = []\n",
    "    y_pred_svm = []\n",
    "    y_pred_tree = []    \n",
    "\n",
    "    if vpn.empty:\n",
    "        \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "\n",
    "        #shuffle the data \n",
    "        indices=np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "        X=X[indices]\n",
    "        y=y[indices]\n",
    "        \n",
    "        rs = ShuffleSplit(n_splits=3, test_size=.3)\n",
    "        for train, test in rs.split(X):\n",
    "\n",
    "            X_train=X[train]\n",
    "            y_train=y[train]\n",
    "        \n",
    "            X_test=X[test]\n",
    "   \n",
    "            y_test = np.append(y_test, y[test])\n",
    "            y_mean = np.append(y_mean, np.mean(y_train))   \n",
    "\n",
    "            pred_lin_new, pred_svm_new=regress(X_train, X_test, y_train, y_test, hypercv)           \n",
    "            y_pred_lin = np.append(y_pred_lin, pred_lin_new)\n",
    "            y_pred_svm=np.append(y_pred_svm, pred_svm_new)\n",
    "            \n",
    "    \n",
    "    else:\n",
    "\n",
    "        vpns=np.array(vpn.unique())    \n",
    "\n",
    "        rs = ShuffleSplit(n_splits=3, test_size=0.3)\n",
    "        for train, test in rs.split(vpns):\n",
    "        #train, test = train_test_split(vpns, test_size = 0.2)\n",
    "\n",
    "            X_train=np.array(X[vpn.isin(vpns[train])])\n",
    "            X_test=np.array(X[vpn.isin(vpns[test])])     \n",
    "            \n",
    "            y_train=np.array(y[vpn.isin(vpns[train])])\n",
    "            y_test = np.append(y_test, np.array(y[vpn.isin(vpns[test])]))\n",
    "\n",
    "            y_mean = np.append(y_mean, np.mean(y_train)) \n",
    "            \n",
    "            pred_lin_new, pred_svm_new=regress(X_train, X_test, y_train, y_test, hypercv, do_svm=True)       \n",
    "            \n",
    "  \n",
    "            y_pred_lin = np.append(y_pred_lin, pred_lin_new, axis=0)\n",
    "            y_pred_svm = np.append(y_pred_svm, pred_svm_new, axis=0)\n",
    "    \n",
    "    acc_lr, acc_svm, acc_mean=reg_cross_eval(y_mean, y_test, y_pred_lin, y_pred_svm)    \n",
    "    return acc_lr, acc_svm, acc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "def regress(X_train, X_test, y_train, y_test, hypercv, do_svm=False):\n",
    "\n",
    "    pred_svm = []\n",
    "    \n",
    "    #Festlegen der mit Crossvalidierung zu tunenden Parameter\n",
    "    tuned_lin_parameters =[{'alpha':[1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4]}]\n",
    "\n",
    "    tuned_svm_parameters = [{\"C\": [1e0, 1e-1, 1e-2, 1e-3],\n",
    "                            \"gamma\": [1e-2, 1e-1, 1, 1e1, 1e2], #only for rbf\n",
    "                            'degree':[1,2,3]}] #only for poly\n",
    "    lin_clf=GridSearchCV(linear_model.Ridge(random_state=42), \n",
    "                         tuned_lin_parameters, cv=hypercv)\n",
    "\n",
    "    lin_clf.fit(X_train, y_train)       \n",
    "    pred_lin = lin_clf.predict(X_test)\n",
    "\n",
    "    #Evaluation der Classifier auf den Training-Test-Splits\n",
    "    print(\"Beste Parameter fuer Linear Regression auf einem der Trainingssets:\")\n",
    "    print(lin_clf.best_estimator_)\n",
    "    \n",
    "    svm_clf=GridSearchCV(SVR(kernel='rbf'), cv=hypercv, param_grid=tuned_svm_parameters) \n",
    "\n",
    "    svm_clf.fit(X_train, y_train)        \n",
    "    pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "    print(\"Beste Parameter fuer SVM auf einem der Trainingssets:\")\n",
    "    print(svm_clf.best_estimator_)\n",
    "\n",
    "    return pred_lin, pred_svm;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_cross_eval(y_mean, y_test, y_pred_lin, y_pred_svm):\n",
    "    \n",
    "    #Evaluation der crossvalidierten Ergebnisse \n",
    "    print 'Crossvalidierte Ergebnisse'\n",
    "    # The mean squared error\n",
    "    acc_lr=np.sqrt(np.mean(((y_pred_lin) - y_test) ** 2))\n",
    "    acc_svm=np.sqrt(np.mean(((y_pred_svm) - y_test) ** 2))\n",
    "    acc_mean=np.sqrt(np.mean((np.mean((y_mean)) - y_test) ** 2))\n",
    "\n",
    "    print(\"Root Mean Square Error der LR: %.2f\"\n",
    "        % np.sqrt(np.mean(((y_pred_lin) - y_test) ** 2)))\n",
    "\n",
    "    #Explained variance score: 1 is perfect prediction\n",
    "    #print('Variance score: %.2f' % lr.score(X_test, y_test))\n",
    "    print('Korrelation: %.2f' % np.correlate(y_pred_lin, y_test))\n",
    "    # The coefficients\n",
    "    #print('Coefficients: \\n', lr.coef_)\n",
    "    \n",
    "    print(\"Root Mean Square Error der SVM: %.2f\"\n",
    "          % np.sqrt(np.mean(((y_pred_svm) - y_test) ** 2)))\n",
    "    \n",
    "    print 'Prediction des Means'   \n",
    "    print(\"Root Mean Square Error:  %.2f\"\n",
    "     % np.sqrt(np.mean((np.mean((y_mean)) - y_test) ** 2)))\n",
    "    \n",
    "    return acc_lr, acc_svm, acc_mean;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with Linear Regression and SVR (itembased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'handcrafted'\n",
    "#df=df.fillna(method='ffill')\n",
    "emo_df=df[df['con']==0] #nur Items, bei denen Mitgefuehl angegeben werden musste\n",
    "emo_df = emo_df.reset_index(drop=True) \n",
    "\n",
    "print 'Prediction of empathy with both Muscles (only stimulus-time)'\n",
    "regression(emo_df[handcrafted], y=emo_df['key'], vpn=emo_df['vpn'])\n",
    "\n",
    "print 'Prediction of empathy with both Muscles (only stimulus-time) - seperated by valence'\n",
    "df_pic_pos=df.loc[((df.val==1) & (df.con==0)), :].reset_index(drop=True)\n",
    "y=df_pic_pos.key\n",
    "X=df_pic_pos[handcrafted]\n",
    "regression(X, y, vpn=emo_df['vpn'])\n",
    "\n",
    "df_pic_neg=df.loc[((df.val==0) & (df.con==0)), :].reset_index(drop=True)\n",
    "y=df_pic_neg.key\n",
    "X=df_pic_neg[handcrafted]\n",
    "regression(X, y, vpn=emo_df['vpn'])\n",
    "\n",
    "print 'mimicry'\n",
    "X=pd.DataFrame(np.array(emo_df[cor_s])-np.array(emo_df[zyg_s]))\n",
    "y=emo_df.key\n",
    "regression(X, y, vpn=emo_df['vpn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ACHTUNG itembased ist es eigentlich ein Multiple-Class Classification-Task\n",
    "\n",
    "emo_df=df[df['con']==0] #nur Items, bei denen Mitgefuehl angegeben werden musste\n",
    "emo_df = emo_df.reset_index(drop=True) \n",
    "\n",
    "print 'Prediction of empathy with both Muscles (only stimulus-time)'\n",
    "regression(pd.concat([emo_df[zyg_s], emo_df[cor_s]], axis=1), y=emo_df['key'], vpn=emo_df['vpn'])\n",
    "\n",
    "print 'Prediction of empathy with both Muscles (only stimulus-time) - seperated by valence'\n",
    "df_pic_pos=df[(df.val==1) & (df.con==0)].reset_index(drop=True) \n",
    "df_pic_neg=df[(df.val==0) & (df.con==0)].reset_index(drop=True) \n",
    "y=df_pic_pos.key\n",
    "X=pd.concat([df_pic_pos[zyg_s], df_pic_pos[cor_s]], axis=1)\n",
    "regression(X, y, vpn=emo_df['vpn'])\n",
    "\n",
    "X=pd.concat([df_pic_neg[zyg_s], df_pic_neg[cor_s]], axis=1)\n",
    "y=df_pic_neg.key\n",
    "regression(X, y, vpn=emo_df['vpn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTICIPANT OR PIC BASED PREDICTION!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Prediction of AQ with both Muscles (only stimulus-time) - PARTICIPANTBASED'\n",
    "df_pic=df.loc[df.AQsum.notnull()].groupby(['vpn']).mean()\n",
    "y=df_pic.AQsum\n",
    "X=pd.concat([df_pic[zyg_s], df_pic[cor_s]], axis=1)\n",
    "acc=regression(X, y)\n",
    "\n",
    "print 'Prediction of SPF with both Muscles (only stimulus-time) - PARTICIPANTBASED'\n",
    "df_pic=df.loc[df.SPF_SUM.notnull()].groupby(['vpn']).mean()\n",
    "y=df_pic.SPF_SUM \n",
    "X=pd.concat([df_pic[zyg_s], df_pic[cor_s]], axis=1)\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of correct Anwsers with both Muscles - PARTICIPANTBASED'\n",
    "pd.to_numeric(df.cor)\n",
    "df_pic=df.loc[df.con==1].groupby(['vpn']).mean()\n",
    "y=df_pic.cor\n",
    "X=pd.concat([df_pic[zyg_s], df_pic[cor_s]], axis=1)\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of correct Anwsers with both Muscles - PICTUREBASED'\n",
    "df_pic=df.loc[df.con==1].groupby(['pic']).mean()\n",
    "y=df_pic.cor\n",
    "X=pd.concat([df_pic[zyg_s], df_pic[cor_s]], axis=1)\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of Empathy with both Muscles - Participantbased'\n",
    "df_pic=df.loc[df.con==0].groupby(['vpn']).mean()\n",
    "y=df_pic.key\n",
    "X=pd.concat([df_pic[zyg_s], df_pic[cor_s]], axis=1)\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of Empathy with both Muscles - PICTUREBASED'\n",
    "df_pic=df.loc[df.con==0].groupby(['pic']).mean()\n",
    "y=df_pic.key\n",
    "X=pd.concat([df_pic[zyg_s], df_pic[cor_s]], axis=1)\n",
    "regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'HANDCRAFTED'\n",
    "\n",
    "print 'Prediction of AQ with both Muscles (only stimulus-time) - PARTICIPANTBASED'\n",
    "df_pic=df.loc[df.AQsum.notnull()].groupby(['vpn']).mean()\n",
    "x1=df_pic[zyg_s]\n",
    "x2=df_pic[cor_s]\n",
    "y=df_pic.AQsum\n",
    "X=df_pic[handcrafted]\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of SPF with both Muscles (only stimulus-time) - PARTICIPANTBASED'\n",
    "df_pic=df.loc[df.SPF_SUM.notnull()].groupby(['vpn']).mean()\n",
    "x1=df_pic[zyg_s]\n",
    "x2=df_pic[cor_s]\n",
    "y=df_pic.SPF_SUM \n",
    "X=df_pic[handcrafted]\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of correct Anwsers with both Muscles - PARTICIPANTBASED'\n",
    "pd.to_numeric(df.cor)\n",
    "df_pic=df.loc[df.con==1].groupby(['vpn']).mean()\n",
    "y=df_pic.cor\n",
    "X=df_pic[handcrafted]\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of correct Anwsers with both Muscles - PICTUREBASED'\n",
    "df_pic=df.loc[df.con==1].groupby(['pic']).mean()\n",
    "y=df_pic.cor\n",
    "X=df_pic[handcrafted]\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of Empathy with both Muscles - Participantbased'\n",
    "df_pic=df.loc[df.con==0].groupby(['vpn']).mean()\n",
    "y=df_pic.key\n",
    "X=df_pic[handcrafted]\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of Empathy with both Muscles - PICTUREBASED'\n",
    "df_pic=df.loc[df.con==0].groupby(['pic']).mean()\n",
    "y=df_pic.key\n",
    "X=df_pic[handcrafted]\n",
    "regression(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Prediction with more features for pos and neg Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Prediction of AQ with both Muscles (only stimulus-time) - PARTICIPANTBASED'\n",
    "df_pic_pos=df[(df.val==1) & (df.AQsum.notnull())].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.AQsum.notnull())].groupby(['vpn']).mean()\n",
    "y=df_pic_pos.AQsum\n",
    "X=pd.concat([df_pic_pos[zyg_s], df_pic_pos[cor_s], df_pic_neg[zyg_s], df_pic_neg[cor_s]], axis=1)\n",
    "regression(X, y)\n",
    "\n",
    "#df.loc[(df[group1]==trigger)&(df[idx]==id)]                       \n",
    "                        \n",
    "print 'Prediction of SPF with both Muscles (only stimulus-time) - PARTICIPANTBASED'\n",
    "\n",
    "df_pic_pos=df[(df.val==1) & (df.SPF_Empathy_SUM.notnull())].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.SPF_Empathy_SUM.notnull())].groupby(['vpn']).mean()\n",
    "y=df_pic_pos.SPF_SUM \n",
    "X=pd.concat([df_pic_pos[zyg_s], df_pic_pos[cor_s], df_pic_neg[zyg_s], df_pic_neg[cor_s]], axis=1)\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of correct Anwsers with both Muscles - PARTICIPANTBASED'\n",
    "pd.to_numeric(df_pic_pos.cor)\n",
    "df_pic_pos=df[(df.val==1) & (df.con==1)].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.con==1)].groupby(['vpn']).mean()\n",
    "y=df_pic_pos.cor\n",
    "X=pd.concat([df_pic_pos[zyg_s], df_pic_pos[cor_s], df_pic_neg[zyg_s], df_pic_neg[cor_s]], axis=1)\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of Empathy with both Muscles - Participantbased'\n",
    "df_pic_pos=df[(df.val==1) & (df.con==0)].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.con==0)].groupby(['vpn']).mean()\n",
    "y=df_pic_pos.key\n",
    "X=pd.concat([df_pic_pos[zyg_s], df_pic_pos[cor_s], df_pic_neg[zyg_s], df_pic_neg[cor_s]], axis=1)\n",
    "regression(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'Mimikry'\n",
    "print 'Prediction of AQ with both Muscles (only stimulus-time) - PARTICIPANTBASED'\n",
    "df_pic_pos=df[(df.val==1) & (df.AQsum.notnull())].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.AQsum.notnull())].groupby(['vpn']).mean()\n",
    "y=df_pic_pos.AQsum\n",
    "X=np.hstack([(np.array(df_pic_pos[zyg_s])-np.array(df_pic_pos[cor_s])), \n",
    "             np.array(df_pic_neg[zyg_s])-np.array(df_pic_neg[cor_s])])\n",
    "\n",
    "regression(X, y)\n",
    "\n",
    "#df.loc[(df[group1]==trigger)&(df[idx]==id)]                       \n",
    "                        \n",
    "print 'Prediction of SPF with both Muscles (only stimulus-time) - PARTICIPANTBASED'\n",
    "\n",
    "df_pic_pos=df[(df.val==1) & (df.SPF_Empathy_SUM.notnull())].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.SPF_Empathy_SUM.notnull())].groupby(['vpn']).mean()\n",
    "y=df_pic_pos.SPF_SUM \n",
    "X=np.hstack([(np.array(df_pic_pos[zyg_s])-np.array(df_pic_pos[cor_s])), \n",
    "             np.array(df_pic_neg[zyg_s])-np.array(df_pic_neg[cor_s])])\n",
    "regression(X, y)\n",
    "\n",
    "\n",
    "print 'Prediction of Empathy with both Muscles - Participantbased'\n",
    "df_pic_pos=df[(df.val==1) & (df.con==0)].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.con==0)].groupby(['vpn']).mean()\n",
    "y=df_pic_pos.key\n",
    "X=np.hstack([(np.array(df_pic_pos[zyg_s])-np.array(df_pic_pos[cor_s])), \n",
    "             np.array(df_pic_neg[zyg_s])-np.array(df_pic_neg[cor_s])])\n",
    "regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'HANDCRAFTED'\n",
    "print 'Prediction of AQ with both Muscles (only stimulus-time) - PARTICIPANTBASED'\n",
    "df_pic_pos=df[(df.val==1) & (df.AQsum.notnull())].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.AQsum.notnull())].groupby(['vpn']).mean()\n",
    "y=df_pic_pos.AQsum\n",
    "#X=np.hstack([df_pic_pos[handcrafted], df_pic_neg[handcrafted]])\n",
    "#regression(X, y)\n",
    "\n",
    "#df.loc[(df[group1]==trigger)&(df[idx]==id)]                       \n",
    "                        \n",
    "print 'Prediction of SPF with both Muscles (only stimulus-time) - PARTICIPANTBASED'\n",
    "\n",
    "df_pic_pos=df[(df.val==1) & (df.SPF_Empathy_SUM.notnull())].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.SPF_Empathy_SUM.notnull())].groupby(['vpn']).mean()\n",
    "y=df_pic_pos.SPF_SUM \n",
    "X=pd.concat([df_pic_pos[handcrafted], df_pic_neg[handcrafted]], axis=1)\n",
    "regression(X, y)\n",
    "\n",
    "\n",
    "print 'Prediction of correct Anwsers with both Muscles - PARTICIPANTBASED'\n",
    "pd.to_numeric(df_pic_pos.cor)\n",
    "df_pic_pos=df[(df.val==1) & (df.con==1)].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.con==1)].groupby(['vpn']).mean()\n",
    "y=df_pic_pos.cor\n",
    "X=pd.concat([df_pic_pos[handcrafted], df_pic_neg[handcrafted]], axis=1)\n",
    "regression(X, y)\n",
    "\n",
    "print 'Prediction of Empathy with both Muscles - Participantbased'\n",
    "df_pic_pos=df[(df.val==1) & (df.con==0)].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.con==0)].groupby(['vpn']).mean()\n",
    "y=np.array(df_pic_pos.key)\n",
    "X=pd.concat([df_pic_pos[handcrafted], df_pic_neg[handcrafted]], axis=1)\n",
    "regression(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with LogReg, RT and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def classify(X_train, y_train, hypercv):\n",
    "\n",
    "    #Festlegen der mit Crossvalidierung zu tunenden Parameter\n",
    "    tuned_rt_parameters = [{'max_depth':[1, 2,4,6],\n",
    "                   'min_samples_leaf':[1, 2,4,6]}]\n",
    "\n",
    "    tuned_lm_parameters = [{'C':[0.001, 0.01, 0.1, 1., 10.0,], \n",
    "                  }]\n",
    "\n",
    "    tuned_svm_parameters = [{'C':[0.001, 0.01, 0.1, 1., 10.0],  \n",
    "                  }]\n",
    "\n",
    "    #Random-Forest Classifier mit getunten Hyperparametern        \n",
    "    rf_clf = GridSearchCV(RandomForestClassifier(n_estimators=100, class_weight='balanced'), \n",
    "                                         tuned_rt_parameters, cv=hypercv) #, scoring=score\n",
    "\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    \n",
    "    #Logistic Regression Classifier mit getunten Hyperparametern\n",
    "    lr_clf = GridSearchCV(linear_model.LogisticRegression(class_weight='balanced'),\n",
    "               tuned_lm_parameters, cv=hypercv)#, scoring=score)\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    \n",
    "    #SVM\n",
    "    svm_clf = GridSearchCV(svm.LinearSVC(class_weight='balanced'), \n",
    "                   tuned_svm_parameters, cv=hypercv)#, scoring=score)\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "  \n",
    "    return  rf_clf, lr_clf, svm_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def classify_cross_eval(y_true, y_pred_rf, y_pred_lr, y_pred_svm):\n",
    "\n",
    "    #Evaluation der crossvalidierten Ergebnisse \n",
    "    print 'Crossvalidierte Ergebnisse fuer Random Forest'\n",
    "    print metrics.classification_report(y_true,y_pred_rf)\n",
    "\n",
    "    print 'Crossvalidierte Ergebnisse fuer Logistic Regression'\n",
    "    print metrics.classification_report(y_true,y_pred_lr)\n",
    "\n",
    "    print 'Crossvalidierte Ergebnisse fuer Support Vector Machine'\n",
    "    print metrics.classification_report(y_true,y_pred_svm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def classification(X, y, cvtt=3, hypercv=3):\n",
    "    y_true_all = []\n",
    "    y_pred_lr_all = []\n",
    "    y_pred_rf_all = []\n",
    "    y_pred_svm_all = []\n",
    "    bar_svm=[]\n",
    "    bar_lr=[]\n",
    "    bar_rf=[]\n",
    "\n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    \n",
    "    #shuffle the data \n",
    "    indices=np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X=X[indices]\n",
    "    y=y[indices]\n",
    "\n",
    "    #Nested Cross-Validation    \n",
    "    rs = StratifiedShuffleSplit(n_splits=cvtt, test_size=0.3)\n",
    "    for train, test in rs.split(X, y):\n",
    "        \n",
    "        X_train=X[train]\n",
    "        X_test=X[test]\n",
    "        \n",
    "        y_test=y[test]\n",
    "        y_train=y[train]\n",
    "        \n",
    "        rf_clf, lr_clf, svm_clf=classify(X_train, y_train, hypercv=hypercv)\n",
    "              \n",
    "        evaluate(rf_clf, lr_clf, svm_clf, X_test, y_test)       \n",
    "              \n",
    "        y_true_all = np.append(y_true_all, y_test)\n",
    "        y_pred_rf_all = np.append(y_pred_rf_all, rf_clf.predict(X_test)) \n",
    "        y_pred_lr_all = np.append(y_pred_lr_all, lr_clf.predict(X_test)) \n",
    "        y_pred_svm_all = np.append(y_pred_svm_all, svm_clf.predict(X_test)) \n",
    "        \n",
    "        bar_svm=np.append(bar_svm, np.sum(svm_clf.predict(X_test)==y_test)/np.float(len(y_test)))\n",
    "        bar_lr=np.append(bar_lr, np.sum(lr_clf.predict(X_test)==y_test)/np.float(len(y_test)))\n",
    "        bar_rf=np.append(bar_rf, np.sum(rf_clf.predict(X_test)==y_test)/np.float(len(y_test)))\n",
    "\n",
    "    graphics(bar_svm, bar_lr, bar_rf)\n",
    "    \n",
    "    cvs = StratifiedShuffleSplit(n_splits=5, test_size=0.3)\n",
    "    \n",
    "    train_sizes=np.linspace(.1, 1.0, 4)\n",
    "    title='Learning Curve'\n",
    "    try:\n",
    "        plot_learning_curve(linear_model.LogisticRegression(class_weight='balanced', C=lr_clf.best_estimator_.C),\n",
    "                        title, X, y, ylim=(0.4, 1.01), cv=cvs)\n",
    "    except:\n",
    "        print 'no learning curve for LR available'\n",
    "    try:\n",
    "        plot_learning_curve(svm.LinearSVC(class_weight='balanced', C=svm_clf.best_estimator_.C),\n",
    "            title, X, y, ylim=(0.4, 1.01), cv=cvs)\n",
    "    except:\n",
    "        print 'no learning curve for SVM available'\n",
    "    try:\n",
    "        plot_learning_curve(RandomForestClassifier(n_estimators=100, class_weight='balanced',\n",
    "                                               max_depth=rf_clf.best_estimator_.max_depth, \n",
    "                                               min_samples_leaf=rf_clf.best_estimator_.min_samples_leaf),\n",
    "                        title, X, y, ylim=(0.4, 1.01), cv=cvs)\n",
    "    except:\n",
    "        print 'no learning curve for RT available'\n",
    "    classify_cross_eval(y_true_all, y_pred_rf_all, y_pred_lr_all, y_pred_svm_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graphics(bar_svm, bar_lr, bar_rf):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(1, np.mean(bar_lr), yerr=np.std(bar_lr), color='r', label='LR')#, men_means, width, color='r', yerr=men_std)\n",
    "    ax.bar(2, np.mean(bar_svm), yerr=np.std(bar_svm),color='b', label='SVM')\n",
    "    ax.bar(3, np.mean(bar_rf), yerr=np.std(bar_rf), color='y', label='RF')\n",
    "    \n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy of the Different Classifier')\n",
    "    #ax.set_xticks([1,2,3] + 1 / 2)\n",
    "    #ax.set_xticklabels(('LR', 'SVM', 'RF'))st\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(rf_clf, lr_clf, svm_clf, X_test, y_test):\n",
    "#Evaluation der Classifier auf den Training-Test-Splits\n",
    "        \n",
    "    y_pred_rf=rf_clf.predict(X_test)\n",
    "    y_pred_lr=lr_clf.predict(X_test)\n",
    "    y_pred_svm=svm_clf.predict(X_test)\n",
    "\n",
    "    print(\"Beste Parameter fuer Random Forest auf einem der Trainingssets:\")\n",
    "    print(rf_clf.best_estimator_)\n",
    "\n",
    "    print(\"Werte für den RF auf einem der Trainingssets:\")\n",
    "    print metrics.classification_report(y_test, y_pred_rf)\n",
    "\n",
    "    print(\"Beste Parameter fuer Logistic Regression  auf einem der Trainingssets:\")\n",
    "    print(lr_clf.best_estimator_)\n",
    "\n",
    "    print(\"Werte für die LR auf einem der Trainingssets:\")\n",
    "    print metrics.classification_report(y_test,y_pred_lr)\n",
    "\n",
    "    print(\"Beste Parameter fuer SVM  auf einem der Trainingssets:\")\n",
    "    print(svm_clf.best_estimator_)\n",
    "\n",
    "    print(\"Werte für die SVM auf einem der Trainingssets:\")\n",
    "    print metrics.classification_report(y_test,y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def classification_grouped(X, y, vpn, cvtt=3, hypercv=5):\n",
    "      \n",
    "    y_true_all = []\n",
    "    y_pred_lr_all = []\n",
    "    y_pred_rf_all = []\n",
    "    y_pred_svm_all = []\n",
    "    bar_svm=[]\n",
    "    bar_lr=[]\n",
    "    bar_rf=[]\n",
    "    \n",
    "    vpns=np.array(vpn.unique())    \n",
    "\n",
    "    rs = ShuffleSplit(n_splits=3, test_size=0.3)\n",
    "    for train, test in rs.split(vpns):\n",
    "        \n",
    "        X_train=np.array(X[vpn.isin(vpns[train])])\n",
    "        X_test=np.array(X[vpn.isin(vpns[test])])        \n",
    "        y_test =np.array(y[vpn.isin(vpns[test])])\n",
    "        y_train=np.array(y[vpn.isin(vpns[train])])\n",
    "        \n",
    "        rf_clf, lr_clf, svm_clf=classify(X_train, y_train, hypercv=hypercv)\n",
    "         \n",
    "        evaluate(rf_clf, lr_clf, svm_clf, X_test, y_test)       \n",
    "              \n",
    "        y_true_all = np.append(y_true_all, y_test)\n",
    "        y_pred_rf_all = np.append(y_pred_rf_all, rf_clf.predict(X_test)) \n",
    "        y_pred_lr_all = np.append(y_pred_lr_all, lr_clf.predict(X_test)) \n",
    "        y_pred_svm_all = np.append(y_pred_svm_all, svm_clf.predict(X_test)) \n",
    "        \n",
    "        bar_svm=np.append(bar_svm, np.sum(svm_clf.predict(X_test)==y_test)/np.float(len(y_test)))\n",
    "        bar_lr=np.append(bar_lr, np.sum(lr_clf.predict(X_test)==y_test)/np.float(len(y_test)))\n",
    "        bar_rf=np.append(bar_rf, np.sum(rf_clf.predict(X_test)==y_test)/np.float(len(y_test)))\n",
    "    \n",
    "    #eigentlich muesste man hier die gemittelten best_estimator nehmen\n",
    "\n",
    "    graphics(bar_svm, bar_lr, bar_rf)\n",
    "    \n",
    "    cvs = StratifiedShuffleSplit(n_splits=5, test_size=0.3)\n",
    "                              \n",
    "    train_sizes=np.linspace(.1, 1.0, 5)\n",
    "    title='Learning Curve'\n",
    "    \n",
    "    try:\n",
    "        plot_learning_curve(linear_model.LogisticRegression(class_weight='balanced', C=lr_clf.best_estimator_.C),\n",
    "                        title, X, y, (0.4, 1.01), cv=cvs, train_sizes=train_sizes) \n",
    "    \n",
    "    except:\n",
    "         print 'no learning curve for LR available'\n",
    "    try:\n",
    "        plot_learning_curve(RandomForestClassifier(n_estimators=100, class_weight='balanced',\n",
    "                                               max_depth=rf_clf.best_estimator_.max_depth, \n",
    "                                               min_samples_leaf=rf_clf.best_estimator_.min_samples_leaf),\n",
    "                        title, X, y, (0.4, 1.01), cv=cvs)\n",
    "    except:\n",
    "        print 'no learning curve for RF available'\n",
    "    classify_cross_eval(y_true_all, y_pred_rf_all, y_pred_lr_all, y_pred_svm_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (Valence, Condition Correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.dropna(subset=zyg_s).reset_index(drop=True)\n",
    "df=df.dropna(subset=cor_s).reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    df.loc[(df['val']=='pos'), 'val']=1\n",
    "    df.loc[(df['val']=='neg'), 'val']=0\n",
    "except:\n",
    "    print'valence already recoded'\n",
    "    \n",
    "try:\n",
    "    df.loc[(df['con']==2), 'con']=0\n",
    "except:\n",
    "    print'condition already recoded'\n",
    "       \n",
    "df['con']=pd.to_numeric(df.con)\n",
    "df['val']=pd.to_numeric(df.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valence of a Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Prediction of valence with both Muscles'\n",
    "X=pd.concat([df[zyg_s], df[cor_s]], axis=1)\n",
    "y=df['val']\n",
    "vpns=df['vpn']\n",
    "classification_grouped(X, y, vpns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Prediction of valence with handcrafted features'\n",
    "X=df[handcrafted]\n",
    "y=df['val']\n",
    "vpns=df['vpn']\n",
    "classification_grouped(X,y, vpns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### picture-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Prediction of valence with both Muscles (pic-based)'\n",
    "df_pic=df.groupby(['pic']).mean().reset_index()\n",
    "x1=df_pic[zyg_s]\n",
    "x2=df_pic[cor_s]\n",
    "y=df_pic['val']\n",
    "classification(pd.DataFrame(pd.concat([x1, x2], axis=1)), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Handcrafted'\n",
    "print 'Prediction of valence with both Muscles (pic-based)'\n",
    "df_pic=df.groupby(['pic']).mean().reset_index(drop=True)\n",
    "y=df_pic['val']\n",
    "classification(df_pic[handcrafted], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition of a Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Prediction of condition with both Muscles'\n",
    "X=pd.concat([df[zyg_s], df[cor_s]], axis=1)\n",
    "y=df['con']\n",
    "vpns=df['vpn']\n",
    "classification_grouped(X,y, vpns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Prediction of condition with both Muscles'\n",
    "X=df[handcrafted]\n",
    "y=df['con']\n",
    "vpns=df['vpn']\n",
    "classification_grouped(X,y, vpns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'PICTUREBASED'\n",
    "print 'Prediction of condition with both Muscles (pic-based)'\n",
    "cog_df=df[df.con==1].groupby(['pic']).mean().reset_index()\n",
    "emo_df=df[df.con==0].groupby(['pic']).mean().reset_index()\n",
    "Xz=pd.concat([cog_df[zyg_s], emo_df[zyg_s]], axis=0)\n",
    "Xc=pd.concat([cog_df[cor_s], emo_df[cor_s]], axis=0)\n",
    "X=pd.concat([Xz, Xc], axis=1).reset_index(drop=True)\n",
    "y=pd.concat([cog_df['con'], emo_df['con']], axis=0).reset_index(drop=True)\n",
    "vpns=pd.concat([cog_df['pic'], emo_df['pic']], axis=0).reset_index(drop=True)\n",
    "classification_grouped(X,y, vpns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Handcrafted'\n",
    "print 'Prediction of condition with both Muscles (pic-based)'\n",
    "cog_df=pd.DataFrame(df[df.con==1]).groupby(['pic']).mean().reset_index()\n",
    "emo_df=pd.DataFrame(df[df.con==0]).groupby(['pic']).mean().reset_index()\n",
    "X=pd.concat([cog_df[handcrafted], emo_df[handcrafted]], axis=0)\n",
    "y=pd.concat([cog_df['con'], emo_df['con']], axis=0)\n",
    "vpns=pd.concat([cog_df['pic'], emo_df['pic']], axis=0)\n",
    "classification_grouped(X,y, vpns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print 'Prozentsatz korrekt beantworte Items' \n",
    "#print np.divide(sum(cog_df['cor']==1),float(sum(cog_df['cor']==0)+sum(cog_df['cor']==1)))\n",
    "\n",
    "print 'Prediction of correct Anwsers by muscle activity'\n",
    "cog_df=df[df.con==1].reset_index(drop=True)\n",
    "X=pd.concat([cog_df[zyg_s], cog_df[cor_s]], axis=1)\n",
    "y=cog_df['cor']\n",
    "vpns=cog_df['vpn']\n",
    "classification_grouped(X,y, vpns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Handcrafted'\n",
    "print 'Prediction of correct Anwsers by muscle activity'\n",
    "cog_df=df[df.con==1].reset_index(drop=True)\n",
    "X=cog_df[handcrafted]\n",
    "y=cog_df['cor']\n",
    "vpns=cog_df['vpn']\n",
    "classification_grouped(X,y, vpns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Test of Prediction vs. Baseline-Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Prediction of Empathy with both Muscles - PICTUREBASED'\n",
    "df_pic=df.loc[df.con==0].groupby(['pic']).mean()\n",
    "y=df_pic.key\n",
    "X=pd.concat([df_pic[zyg_s], df_pic[cor_s]], axis=1)\n",
    "\n",
    "error_lin_all=[]\n",
    "error_svm_all=[]\n",
    "error_mean_all=[]\n",
    "\n",
    "for i in np.arange(1000):\n",
    "    error_lin, error_svm, error_mean=regression(X, y)\n",
    "    error_lin_all = np.append(error_lin_all, error_lin)\n",
    "    error_svm_all = np.append(error_svm_all, error_svm)\n",
    "    error_mean_all = np.append(error_mean_all, error_mean)\n",
    "    \n",
    "import scipy\n",
    "print scipy.stats.ttest_rel(error_lin_all, error_mean_all)\n",
    "print scipy.stats.ttest_rel(error_svm_all, error_mean_all)\n",
    "print np.nanmean(error_svm_all)\n",
    "print np.nanmean(error_lin_all)\n",
    "print np.nanmean(error_mean_all)\n",
    "\n",
    "print np.var(error_lin_all)\n",
    "print np.var(error_svm_all)\n",
    "print scipy.stats.wilcoxon(error_lin_all, error_mean_all, zero_method='wilcox', correction=False)\n",
    "print scipy.stats.wilcoxon(error_svm_all, error_mean_all, zero_method='wilcox', correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print np.sum(error_lin_all>error_mean_all)\n",
    "print np.sum(error_svm_all>error_mean_all)\n",
    "print np.std(error_lin_all)\n",
    "print np.std(error_svm_all)\n",
    "print np.std(error_mean_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Prediction of AQ with both Muscles (only stimulus-time) - PARTICIPANTBASED'\n",
    "df_pic_pos=df[(df.val==1) & (df.AQsum.notnull())].groupby(['vpn']).mean()\n",
    "df_pic_neg=df[(df.val==0) & (df.AQsum.notnull())].groupby(['vpn']).mean()\n",
    "y=df_pic_pos.AQsum\n",
    "X=pd.concat([df_pic_pos[zyg_s], df_pic_pos[cor_s], df_pic_neg[zyg_s], df_pic_neg[cor_s]], axis=1)\n",
    "\n",
    "error_lin_all=[]\n",
    "error_svm_all=[]\n",
    "error_mean_all=[]\n",
    "\n",
    "for i in np.arange(1000):\n",
    "    error_lin, error_svm, error_mean=regression(X, y)\n",
    "    error_lin_all = np.append(error_lin_all, error_lin)\n",
    "    error_svm_all = np.append(error_svm_all, error_svm)\n",
    "    error_mean_all = np.append(error_mean_all, error_mean)\n",
    "    \n",
    "import scipy\n",
    "print scipy.stats.ttest_rel(error_lin_all, error_mean_all)\n",
    "print scipy.stats.ttest_rel(error_svm_all, error_mean_all)\n",
    "print np.nanmean(error_svm_all)\n",
    "print np.nanmean(error_lin_all)\n",
    "print np.nanmean(error_mean_all)\n",
    "print np.var(error_lin_all)\n",
    "print np.var(error_svm_all)\n",
    "print scipy.stats.wilcoxon(error_lin_all, error_mean_all, zero_method='wilcox', correction=False)\n",
    "print scipy.stats.wilcoxon(error_svm_all, error_mean_all, zero_method='wilcox', correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print np.std(error_lin_all)\n",
    "print np.std(error_svm_all)\n",
    "print np.std(error_mean_all)\n",
    "np.sum(error_svm_all>error_mean_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Prediction of condition with Mimicry'\n",
    "X=np.array(df[zyg_s])-np.array(df[cor_s])\n",
    "y=np.array(df['con'])\n",
    "vpns=df['vpn']\n",
    "classification_grouped(X,y, vpns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Prediction of correct Anwsers by Mimicry'\n",
    "cog_df=df[df.con==1].reset_index(drop=True)\n",
    "X=np.array(cog_df[zyg_s])-np.array(cog_df[cor_s])\n",
    "y=np.array(cog_df['cor'])\n",
    "vpns=cog_df['vpn']\n",
    "classification_grouped(X,y, vpns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import DTW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def d_DTW(x, x2, dist):\n",
    "    t1, t2 = len(x), len(x2)\n",
    "    \n",
    "    if x == [] and x2 == []:\n",
    "        return 0.0\n",
    "    elif (x == []) or (x2 == []):\n",
    "        return np.infty\n",
    "    \n",
    "    dp = np.empty((t1+1, t2+1))    \n",
    "    dp[0, 0] = 0\n",
    "    \n",
    "    for i in xrange(1, t1+1):\n",
    "        dp[i, 0] = np.infty\n",
    "    \n",
    "    for j in xrange(1, t2+1):\n",
    "        dp[0, j] = np.infty\n",
    "        \n",
    "    for k in xrange(1, t1+t2+1):\n",
    "        for i in xrange(max(1, k - t2), min(k, t1+1)):\n",
    "            j = k - i      \n",
    "            dp[i, j] = dist(x[i-1], x2[j-1]) + min(dp[i-1, j-1], dp[i, j-1], dp[i-1, j])\n",
    "            # print i, j, dp[i, j]\n",
    "    \n",
    "    return dp[t1, t2]\n",
    "\n",
    "\n",
    "def d1(x, x2):\n",
    "    return 0 if x != x2 else 1\n",
    "\n",
    "def d2(x, x2):\n",
    "    return (x - x2)**2\n",
    "\n",
    "def d3(x, x2):\n",
    "    return abs(x - x2)\n",
    "\n",
    "\n",
    "def build_dtw_gram_matrix(xs, x2s, k):\n",
    "    \"\"\"\n",
    "    xs: collection of sequences (vectors of possibly varying length)\n",
    "    x2s: the same, needed for prediction\n",
    "    k: a kernel function that maps two sequences of possibly different length to a real\n",
    "    The function returns the Gram matrix with respect to k of the data xs.\n",
    "    \"\"\"\n",
    "    t1, t2 = len(xs), len(x2s)\n",
    "    K = np.empty((t1, t2))\n",
    "    \n",
    "    for i in xrange(t1):\n",
    "        for j in xrange(i, t2):\n",
    "            K[i, j] = k(xs[i], x2s[j])\n",
    "            if i < t2 and j < t1:\n",
    "                K[j, i] = K[i, j]\n",
    "        \n",
    "    return K\n",
    "    \n",
    "    \n",
    "def L2_reg(w, lbda):\n",
    "    return 0.5 * lbda * (np.dot(w.T, w)), lbda*w\n",
    "\n",
    "def hinge_loss(h, y):\n",
    "    n = len(h)\n",
    "    l = np.maximum(0, np.ones(n) - y*h)\n",
    "    g = -y * (h > 0)\n",
    "    return l, g\n",
    "\n",
    "\n",
    "def learn_reg_kernel_ERM(X, y, lbda, k, loss=hinge_loss, reg=L2_reg, max_iter=200, tol=0.001, eta=1., verbose=False):\n",
    "    \"\"\"Kernel Linear Regression (default: kernelized L_2 SVM)\n",
    "    X -- data, each row = instance\n",
    "    y -- vector of labels, n_rows(X) == y.shape[0]\n",
    "    lbda -- regularization coefficient lambda\n",
    "    k -- the kernel function\n",
    "    loss -- loss function, returns vector of losses (for each instance) AND the gradient\n",
    "    reg -- regularization function, returns reg-loss and gradient\n",
    "    max_iter -- max. number of iterations of gradient descent\n",
    "    tol -- stop if norm(gradient) < tol\n",
    "    eta -- learning rate\n",
    "    \"\"\"\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    g_old = None\n",
    "    \n",
    "    K = build_dtw_gram_matrix(X, X, k) #\n",
    "    w = np.random.randn(K.shape[0]) #w = np.random.randn(num_features)\n",
    "    \n",
    "    for _ in xrange(max_iter):\n",
    "        h = np.dot(K, w) # h = np.dot(X, w)\n",
    "        l,lg = loss(h, y)\n",
    "        \n",
    "        if verbose:\n",
    "            print 'training loss: {}'.format(np.mean(l))\n",
    "            \n",
    "        r,rg = reg(w, lbda)\n",
    "        g = lg + rg \n",
    "        \n",
    "        if g_old is not None:\n",
    "            eta = eta*(np.dot(g_old.T,K).dot(g_old))/(np.dot((g_old - g).T, K).dot(g_old))\n",
    "            # eta = eta*(np.dot(g_old.T,g_old))/(np.dot((g_old - g).T, g_old))\n",
    "            \n",
    "        w = w - eta*g\n",
    "        if (np.linalg.norm(eta*g)<tol):\n",
    "            break\n",
    "        g_old = g\n",
    "        \n",
    "    return w, K\n",
    "\n",
    "\n",
    "def predict(alpha, X, X_train, k):\n",
    "    K = build_dtw_gram_matrix(X_train, X, k)\n",
    "    y_pred = np.dot(K, alpha)\n",
    "    y_pred[y_pred >= 0] = 1\n",
    "    y_pred[y_pred < 0] = -1\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1,\n",
    "                        train_sizes=10, # list of floats that describe ratio of test data sets tried\n",
    "                        # OR an int = # how many trials\n",
    "                        scoring=None):\n",
    "\n",
    "    if type(train_sizes) == int:\n",
    "        train_sizes=np.linspace(.1, 1.0, train_sizes)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    " \n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    if cv is not None:\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    if cv is not None:\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class KernelEstimator(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, k, lbda):\n",
    "        self.k = k\n",
    "        self.lbda = lbda\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self._X_train = X\n",
    "        self._alpha, _ = learn_reg_kernel_ERM(X, y, lbda=self.lbda, k=self.k, max_iter=20000, eta=1, tol=1e-3)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return predict(self._alpha, self._X_train, X, self.k)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y == y_pred)\n",
    "        \n",
    "\n",
    "def dtwkernel(X,y):\n",
    "    k1_hyp, k2_hyp, k3_hyp = [lambda lmbd: (lambda x, x2: np.exp(-lmbd * DTW.d_DTW(x, x2, d))) for d in [DTW.d1, DTW.d2, DTW.d3]]\n",
    "    k1 = k1_hyp(2.0)\n",
    "    k2 = k2_hyp(2.0)\n",
    "    k3 = k3_hyp(2.0)\n",
    "\n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    alpha, K = DTW.learn_reg_kernel_ERM(X_train, y_train, lbda=1, k=k2, max_iter=500, eta=1, tol=1e-3, verbose=True)\n",
    "    \n",
    "    y_pred = DTW.predict(alpha, X_train, X_train, k2)\n",
    "    print \"Training Accuracy: {}\".format(np.mean(y_train == y_pred))\n",
    "    print \"Test Accuracy: {}\".format(np.mean(y_test == DTW.predict(alpha, X_train, X_test, k2)))\n",
    "    \n",
    "    estimator = DTW.KernelEstimator(k2, 2.0)\n",
    "    DTW.estimator.fit(X_train, y_train)\n",
    "    print \"Accuracy {}\".format(estimator.score(X_train, y_train))\n",
    "    \n",
    "\n",
    "#PREDICTION OF THE VALENCE OF A PIC (averaged above participants)\n",
    "print 'Prediction of valence with both Muscles'\n",
    "#df.loc[(df['val']=='pos'), 'val']=1\n",
    "#df.loc[(df['val']=='neg'), 'val']=0\n",
    "\n",
    "df['val']=pd.to_numeric(df.val)\n",
    "df_nn=df.dropna(subset=zyg_s)\n",
    "df_pic=df.groupby(['pic']).mean().reset_index()\n",
    "x=pd.DataFrame(np.array(df_nn[cor_s])-np.array(df_nn[zyg_s]))\n",
    "y=df_nn['val']\n",
    "\n",
    "#print y.dropna(subset=x)\n",
    "dtwkernel(x,y)\n",
    "\n",
    "print 'Prediction of correct Anwsers by muscle activity'\n",
    "#cog_df=(df.loc[df['con']==1])   \n",
    "#cog_df = cog_df.reset_index(drop=True)\n",
    "\n",
    "#y=np.array(cog_df.cor)\n",
    "#x=cog_df[zyg_s]\n",
    "#dtwkernel(x,y)\n",
    "\n",
    "#x=df_pic[cor_s]\n",
    "#dtwkernel(X,y)\n",
    "\n",
    "\n",
    "# estimator = DTW.KernelEstimator(k2, 2.0)\n",
    "#DTW.estimator.fit(X_train, y_train)\n",
    "#print \"Accuracy {}\".format(estimator.score(X_train, y_train))\n",
    "\n",
    "#DTW.plot_learning_curve(KernelEstimator(k2, 2.0), 'Euclidean distance DTW, lambda = 1.0', X_train, y_train, cv=None, scoring=\"accuracy\", train_sizes=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
